# CMBS Data Pipeline

A comprehensive data pipeline for extracting, processing, and analyzing Commercial Mortgage-Backed Securities (CMBS) data from SEC EDGAR ABS-EE filings.

## Project Overview

This project creates an end-to-end data pipeline for CMBS loan-level data analysis, enabling investors and analysts to access structured, analysis-ready CMBS data from SEC filings.

### Current Status: Phase 1 Complete âœ…

- âœ… SEC EDGAR scraping system for CMBS ABS-EE filings
- âœ… PySpark-based XML processing and validation
- âœ… Data quality profiling and metadata generation
- âœ… Comprehensive test coverage
- âœ… Configurable trust/issuer targeting

### Next Phase: Data Warehouse & Transformations ğŸš§

- ğŸ”„ DBT models for data transformation
- ğŸ”„ Data warehouse setup (Snowflake/PostgreSQL)
- ğŸ”„ Data quality tests and documentation
- ğŸ”„ Metric definitions and business logic

### Future Phases ğŸ“‹

- ğŸ“Š Lightdash integration for visualization
- ğŸ”„ Apache Airflow orchestration
- ğŸŒ FastAPI service layer
- ğŸ“ˆ Advanced analytics models

## Project Structure

```
SECPipeline/
â”œâ”€â”€ spark_processors/     # PySpark processing modules
â”‚   â”œâ”€â”€ xml_processor.py  # XML parsing and transformation
â”‚   â”œâ”€â”€ data_profiler.py # Data quality analysis
â”‚   â””â”€â”€ schema_manager.py # Schema validation
â”œâ”€â”€ utils/               # Utility modules
â”‚   â”œâ”€â”€ cmbs_scraper.py # SEC EDGAR scraping
â”‚   â””â”€â”€ config.py       # Configuration management
â”œâ”€â”€ tests/              # Test suite
â”‚   â””â”€â”€ ...            # Comprehensive tests
â”œâ”€â”€ dbt/               # [Coming Soon] DBT models
â”œâ”€â”€ airflow-docker/    # [Coming Soon] Airflow DAGs
â””â”€â”€ api/               # [Coming Soon] FastAPI service
```

## Current Features

### Data Extraction
- Automated scraping of CMBS ABS-EE filings from SEC EDGAR
- Configurable company/trust targeting
- Rate-limited requests compliant with SEC guidelines
- Robust error handling and retry logic

### Data Processing
- Scalable XML processing using PySpark
- Schema validation and enforcement
- Data quality profiling and metadata generation
- Efficient storage format conversion

### Quality Assurance
- Comprehensive test suite
- Data validation rules
- Error logging and monitoring
- Performance optimization

## Upcoming Features

### Data Warehouse (Next Phase)
- Scalable data storage in Snowflake/PostgreSQL
- Incremental loading patterns
- Historical data tracking
- Efficient partitioning strategy

### DBT Implementation (Next Phase)
- Modular transformation models
- Data quality tests
- Documentation
- Metric definitions

### Future Enhancements
- Lightdash dashboards and visualizations
- Airflow-orchestrated workflows
- RESTful API access
- Advanced analytics capabilities

## Installation

1. Clone the repository:
```bash
git clone [repository-url]
```

2. Install core dependencies:
```bash
# For Spark processing
pip install -r spark_processors/requirements.txt

# For utilities
pip install -r utils/requirements.txt

# For testing
pip install -r tests/requirements.txt
```

## Usage

### Data Extraction
```python
from utils.cmbs_scraper import CMBSScraper

scraper = CMBSScraper()
scraper.get_search_table("Bank of America Merrill Lynch")
scraper.get_company_table()
```

### Data Processing
```python
from spark_processors.xml_processor import XMLProcessor

processor = XMLProcessor()
processor.process_directory("path/to/xml/files")
```

## Development Roadmap

1. **Current Phase (Complete)**
   - âœ… SEC EDGAR scraping
   - âœ… XML processing
   - âœ… Data validation
   - âœ… Testing

2. **Next Phase (Q1 2025)**
   - Data warehouse setup
   - DBT model development
   - Data quality framework
   - Documentation

3. **Future Phases (Q2-Q3 2025)**
   - Visualization layer
   - API development
   - Workflow orchestration
   - Advanced analytics

## Testing

Run the test suite:
```bash
pytest
```

For coverage report:
```bash
pytest --cov=spark_processors --cov=utils
```

## Contact

mail@alferov-dmitry.ru